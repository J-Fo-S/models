{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenet_example_mod.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "T_cETKXHDTXu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/J-Fo-S/models/blob/master/mobilenet_example_mod01.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "aUVxY7xOGD1G",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        ">[Prerequisites (downloading tensorflow_models and checkpoints)](#scrollTo=T_cETKXHDTXu)\n",
        "\n",
        ">[Checkpoint based inference](#scrollTo=fxMe7_pkk_Vo)\n",
        "\n",
        ">[Adversarial Hack](#scrollTo=PlwvpK3ElBk6)\n",
        "\n",
        "https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\n"
      ]
    },
    {
      "metadata": {
        "id": "T_cETKXHDTXu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prerequisites (downloading tensorflow_models and checkpoints)"
      ]
    },
    {
      "metadata": {
        "id": "zo5GyseklSVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb265cfa-a4d6-4a70-8105-3522ddcba8dc"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": 713,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "obaW6O8bz3mA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "765f923c-ef5b-4272-ed54-3fb272a278a2"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from IPython import display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "checkpoint_name = 'mobilenet_v2_1.0_224' #@param\n",
        "url = 'https://storage.googleapis.com/mobilenet_v2/checkpoints/' + checkpoint_name + '.tgz'\n",
        "print('Downloading from ', url)\n",
        "!wget {url}\n",
        "print('Unpacking')\n",
        "!tar -xvf {checkpoint_name}.tgz\n",
        "checkpoint = checkpoint_name + '.ckpt'\n",
        "\n",
        "display.clear_output()\n",
        "print('Successfully downloaded checkpoint from ', url,\n",
        "      '. It is available as', checkpoint)\n"
      ],
      "execution_count": 714,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded checkpoint from  https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz . It is available as mobilenet_v2_1.0_224.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qZDfLegf3hpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "17fa233f-94dc-41dc-cfb0-0480be753d02"
      },
      "cell_type": "code",
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG -O panda.jpg"
      ],
      "execution_count": 715,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-07-11 19:10:13--  https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG\r\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 208.80.154.240, 2620:0:861:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|208.80.154.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116068 (113K) [image/jpeg]\n",
            "Saving to: ‘panda.jpg’\n",
            "\n",
            "panda.jpg           100%[===================>] 113.35K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2018-07-11 19:10:14 (3.31 MB/s) - ‘panda.jpg’ saved [116068/116068]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g0H2RDadndug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setup path\n",
        "import sys\n",
        "sys.path.append('/content/models/research/slim')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fxMe7_pkk_Vo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checkpoint based inference"
      ]
    },
    {
      "metadata": {
        "id": "GrQemT66CxXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "c3dc3c59-ed97-41ee-d594-be8e4d297bc3"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from nets.mobilenet import mobilenet_v2\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# For simplicity we just decode jpeg inside tensorflow.\n",
        "# But one can provide any input obviously.\n",
        "file_input = tf.placeholder(tf.string, (), name=\"input\")\n",
        "\n",
        "image = tf.image.decode_jpeg(tf.read_file(file_input))\n",
        "#image_dec = np.asarray(PIL.Image.open('panda.jpg'))\n",
        "\n",
        "images = tf.expand_dims(image, 0)\n",
        "images = tf.cast(images, tf.float32) / 128.  - 1\n",
        "images.set_shape((None, None, None, 3))\n",
        "images = tf.image.resize_images(images, (224, 224))\n",
        "\n",
        "\n",
        "# Note: arg_scope is optional for inference.\n",
        "with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=False)):\n",
        "  logits, endpoints = mobilenet_v2.mobilenet(images)\n",
        "  pl_cls_target = tf.placeholder(dtype=tf.int32)\n",
        "\n",
        "  # Add a new loss-function. This is the cross-entropy.\n",
        "  # See Tutorial #01 for an explanation of cross-entropy.\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=[pl_cls_target])\n",
        "\n",
        "  # Get the gradient for the loss-function with regard to\n",
        "  # the resized input image.\n",
        "  #off_x = endpoints['layer_1'].eval(feed_dict={file_input: 'panda.jpg'})\n",
        "  \n",
        "  #t_off_x = tf.get_variable(off_x, name='off_x')\n",
        "  \n",
        "  gradient = tf.gradients(loss, endpoints['layer_1'])\n",
        "  print(tf.get_default_graph())\n",
        "  print(gradient)\n",
        "  print(tf.get_default_graph())\n",
        "  \n",
        "# Restore using exponential moving average since it produces (1.5-2%) higher \n",
        "# accuracy\n",
        "ema = tf.train.ExponentialMovingAverage(0.999)\n",
        "vars = ema.variables_to_restore()\n",
        "\n",
        "saver = tf.train.Saver(vars)  "
      ],
      "execution_count": 717,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.framework.ops.Graph object at 0x7fef2f17b860>\n",
            "[<tf.Tensor 'gradients/MobilenetV2/expanded_conv/depthwise/depthwise_grad/DepthwiseConv2dNativeBackpropInput:0' shape=(1, 112, 112, 32) dtype=float32>]\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7fef2f17b860>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TJbLYo_FCxXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c3114d06-54b5-4947-8ed7-aa2c411d2b5b"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from IPython import display\n",
        "import pylab\n",
        "from datasets import imagenet\n",
        "import PIL\n",
        "display.display(display.Image('panda.jpg'))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess,  checkpoint)\n",
        "  x = endpoints['Predictions'].eval(feed_dict={file_input: 'panda.jpg'})\n",
        "  print(tf.get_default_graph())\n",
        "  \n",
        "label_map = imagenet.create_readable_names_for_imagenet_labels()  \n",
        "print(\"Top 1 prediction: \", x.argmax(),label_map[x.argmax()], x.max())\n",
        "\"\"\""
      ],
      "execution_count": 718,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom IPython import display\\nimport pylab\\nfrom datasets import imagenet\\nimport PIL\\ndisplay.display(display.Image(\\'panda.jpg\\'))\\n\\nwith tf.Session() as sess:\\n  saver.restore(sess,  checkpoint)\\n  x = endpoints[\\'Predictions\\'].eval(feed_dict={file_input: \\'panda.jpg\\'})\\n  print(tf.get_default_graph())\\n  \\nlabel_map = imagenet.create_readable_names_for_imagenet_labels()  \\nprint(\"Top 1 prediction: \", x.argmax(),label_map[x.argmax()], x.max())\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 718
        }
      ]
    },
    {
      "metadata": {
        "id": "PlwvpK3ElBk6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Adversarial Hack"
      ]
    },
    {
      "metadata": {
        "id": "lIgqKf2eQdv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce6e5de8-e7b7-4b49-ebc4-20b446b2fc12"
      },
      "cell_type": "code",
      "source": [
        "print(logits)"
      ],
      "execution_count": 719,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"MobilenetV2/Logits/output:0\", shape=(1, 1001), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0n8yaqcyP8mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8f34f1eb-b7ac-4dbd-865c-13773078f29a"
      },
      "cell_type": "code",
      "source": [
        "print(endpoints)"
      ],
      "execution_count": 720,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'layer_1': <tf.Tensor 'MobilenetV2/Conv/Relu6:0' shape=(1, 112, 112, 32) dtype=float32>, 'layer_2': <tf.Tensor 'MobilenetV2/expanded_conv/output:0' shape=(1, 112, 112, 16) dtype=float32>, 'layer_3': <tf.Tensor 'MobilenetV2/expanded_conv_1/output:0' shape=(1, 56, 56, 24) dtype=float32>, 'layer_4': <tf.Tensor 'MobilenetV2/expanded_conv_2/output:0' shape=(1, 56, 56, 24) dtype=float32>, 'layer_5': <tf.Tensor 'MobilenetV2/expanded_conv_3/output:0' shape=(1, 28, 28, 32) dtype=float32>, 'layer_6': <tf.Tensor 'MobilenetV2/expanded_conv_4/output:0' shape=(1, 28, 28, 32) dtype=float32>, 'layer_7': <tf.Tensor 'MobilenetV2/expanded_conv_5/output:0' shape=(1, 28, 28, 32) dtype=float32>, 'layer_8': <tf.Tensor 'MobilenetV2/expanded_conv_6/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_9': <tf.Tensor 'MobilenetV2/expanded_conv_7/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_10': <tf.Tensor 'MobilenetV2/expanded_conv_8/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_11': <tf.Tensor 'MobilenetV2/expanded_conv_9/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_12': <tf.Tensor 'MobilenetV2/expanded_conv_10/output:0' shape=(1, 14, 14, 96) dtype=float32>, 'layer_13': <tf.Tensor 'MobilenetV2/expanded_conv_11/output:0' shape=(1, 14, 14, 96) dtype=float32>, 'layer_14': <tf.Tensor 'MobilenetV2/expanded_conv_12/output:0' shape=(1, 14, 14, 96) dtype=float32>, 'layer_15': <tf.Tensor 'MobilenetV2/expanded_conv_13/output:0' shape=(1, 7, 7, 160) dtype=float32>, 'layer_16': <tf.Tensor 'MobilenetV2/expanded_conv_14/output:0' shape=(1, 7, 7, 160) dtype=float32>, 'layer_17': <tf.Tensor 'MobilenetV2/expanded_conv_15/output:0' shape=(1, 7, 7, 160) dtype=float32>, 'layer_18': <tf.Tensor 'MobilenetV2/expanded_conv_16/output:0' shape=(1, 7, 7, 320) dtype=float32>, 'layer_19': <tf.Tensor 'MobilenetV2/Conv_1/Relu6:0' shape=(1, 7, 7, 1280) dtype=float32>, 'layer_2/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv/depthwise_output:0' shape=(1, 112, 112, 32) dtype=float32>, 'layer_2/output': <tf.Tensor 'MobilenetV2/expanded_conv/output:0' shape=(1, 112, 112, 16) dtype=float32>, 'layer_3/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_1/expansion_output:0' shape=(1, 112, 112, 96) dtype=float32>, 'layer_3/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_1/depthwise_output:0' shape=(1, 56, 56, 96) dtype=float32>, 'layer_3/output': <tf.Tensor 'MobilenetV2/expanded_conv_1/output:0' shape=(1, 56, 56, 24) dtype=float32>, 'layer_4/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_2/expansion_output:0' shape=(1, 56, 56, 144) dtype=float32>, 'layer_4/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_2/depthwise_output:0' shape=(1, 56, 56, 144) dtype=float32>, 'layer_4/output': <tf.Tensor 'MobilenetV2/expanded_conv_2/output:0' shape=(1, 56, 56, 24) dtype=float32>, 'layer_5/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_3/expansion_output:0' shape=(1, 56, 56, 144) dtype=float32>, 'layer_5/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_3/depthwise_output:0' shape=(1, 28, 28, 144) dtype=float32>, 'layer_5/output': <tf.Tensor 'MobilenetV2/expanded_conv_3/output:0' shape=(1, 28, 28, 32) dtype=float32>, 'layer_6/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_4/expansion_output:0' shape=(1, 28, 28, 192) dtype=float32>, 'layer_6/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_4/depthwise_output:0' shape=(1, 28, 28, 192) dtype=float32>, 'layer_6/output': <tf.Tensor 'MobilenetV2/expanded_conv_4/output:0' shape=(1, 28, 28, 32) dtype=float32>, 'layer_7/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_5/expansion_output:0' shape=(1, 28, 28, 192) dtype=float32>, 'layer_7/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_5/depthwise_output:0' shape=(1, 28, 28, 192) dtype=float32>, 'layer_7/output': <tf.Tensor 'MobilenetV2/expanded_conv_5/output:0' shape=(1, 28, 28, 32) dtype=float32>, 'layer_8/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_6/expansion_output:0' shape=(1, 28, 28, 192) dtype=float32>, 'layer_8/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_6/depthwise_output:0' shape=(1, 14, 14, 192) dtype=float32>, 'layer_8/output': <tf.Tensor 'MobilenetV2/expanded_conv_6/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_9/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_7/expansion_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_9/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_7/depthwise_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_9/output': <tf.Tensor 'MobilenetV2/expanded_conv_7/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_10/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_8/expansion_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_10/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_8/depthwise_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_10/output': <tf.Tensor 'MobilenetV2/expanded_conv_8/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_11/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_9/expansion_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_11/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_9/depthwise_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_11/output': <tf.Tensor 'MobilenetV2/expanded_conv_9/output:0' shape=(1, 14, 14, 64) dtype=float32>, 'layer_12/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_10/expansion_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_12/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_10/depthwise_output:0' shape=(1, 14, 14, 384) dtype=float32>, 'layer_12/output': <tf.Tensor 'MobilenetV2/expanded_conv_10/output:0' shape=(1, 14, 14, 96) dtype=float32>, 'layer_13/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_11/expansion_output:0' shape=(1, 14, 14, 576) dtype=float32>, 'layer_13/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_11/depthwise_output:0' shape=(1, 14, 14, 576) dtype=float32>, 'layer_13/output': <tf.Tensor 'MobilenetV2/expanded_conv_11/output:0' shape=(1, 14, 14, 96) dtype=float32>, 'layer_14/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_12/expansion_output:0' shape=(1, 14, 14, 576) dtype=float32>, 'layer_14/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_12/depthwise_output:0' shape=(1, 14, 14, 576) dtype=float32>, 'layer_14/output': <tf.Tensor 'MobilenetV2/expanded_conv_12/output:0' shape=(1, 14, 14, 96) dtype=float32>, 'layer_15/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_13/expansion_output:0' shape=(1, 14, 14, 576) dtype=float32>, 'layer_15/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_13/depthwise_output:0' shape=(1, 7, 7, 576) dtype=float32>, 'layer_15/output': <tf.Tensor 'MobilenetV2/expanded_conv_13/output:0' shape=(1, 7, 7, 160) dtype=float32>, 'layer_16/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_14/expansion_output:0' shape=(1, 7, 7, 960) dtype=float32>, 'layer_16/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_14/depthwise_output:0' shape=(1, 7, 7, 960) dtype=float32>, 'layer_16/output': <tf.Tensor 'MobilenetV2/expanded_conv_14/output:0' shape=(1, 7, 7, 160) dtype=float32>, 'layer_17/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_15/expansion_output:0' shape=(1, 7, 7, 960) dtype=float32>, 'layer_17/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_15/depthwise_output:0' shape=(1, 7, 7, 960) dtype=float32>, 'layer_17/output': <tf.Tensor 'MobilenetV2/expanded_conv_15/output:0' shape=(1, 7, 7, 160) dtype=float32>, 'layer_18/expansion_output': <tf.Tensor 'MobilenetV2/expanded_conv_16/expansion_output:0' shape=(1, 7, 7, 960) dtype=float32>, 'layer_18/depthwise_output': <tf.Tensor 'MobilenetV2/expanded_conv_16/depthwise_output:0' shape=(1, 7, 7, 960) dtype=float32>, 'layer_18/output': <tf.Tensor 'MobilenetV2/expanded_conv_16/output:0' shape=(1, 7, 7, 320) dtype=float32>, 'global_pool': <tf.Tensor 'MobilenetV2/Logits/AvgPool:0' shape=(1, 1, 1, 1280) dtype=float32>, 'Logits': <tf.Tensor 'MobilenetV2/Logits/output:0' shape=(1, 1001) dtype=float32>, 'Predictions': <tf.Tensor 'MobilenetV2/Predictions/Reshape_1:0' shape=(1, 1001) dtype=float32>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wKuXE3rtMuDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2c572608-0231-491f-ec3f-edf516849420"
      },
      "cell_type": "code",
      "source": [
        "# Set the graph for the Inception model as the default graph,\n",
        "# so that all changes inside this with-block are done to that graph.\n",
        "#with model.graph.as_default():\n",
        "\n",
        "#with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n",
        "#     logits, endpoints = mobilenet_base(...)  \n",
        "\n",
        "#new_graph = tf.Graph()\n",
        "\n",
        "#with new_graph.as_default():\n",
        "#with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=False)):\n",
        "\"\"\"\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess,  checkpoint)\n",
        "  # This will be set to e.g. 300 for the 'bookcase' class.\n",
        "  pl_cls_target = tf.placeholder(dtype=tf.int32)\n",
        "\n",
        "  # Add a new loss-function. This is the cross-entropy.\n",
        "  # See Tutorial #01 for an explanation of cross-entropy.\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=[pl_cls_target])\n",
        "\n",
        "  # Get the gradient for the loss-function with regard to\n",
        "  # the resized input image.\n",
        "  #off_x = endpoints['layer_1'].eval(feed_dict={file_input: 'panda.jpg'})\n",
        "  \n",
        "  #t_off_x = tf.get_variable(off_x, name='off_x')\n",
        "  \n",
        "  gradient = tf.gradients(loss, endpoints['layer_1'])\n",
        "  print(tf.get_default_graph())\n",
        "  print(gradient)\n",
        "\"\"\""
      ],
      "execution_count": 721,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwith tf.Session() as sess:\\n  saver.restore(sess,  checkpoint)\\n  # This will be set to e.g. 300 for the 'bookcase' class.\\n  pl_cls_target = tf.placeholder(dtype=tf.int32)\\n\\n  # Add a new loss-function. This is the cross-entropy.\\n  # See Tutorial #01 for an explanation of cross-entropy.\\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=[pl_cls_target])\\n\\n  # Get the gradient for the loss-function with regard to\\n  # the resized input image.\\n  #off_x = endpoints['layer_1'].eval(feed_dict={file_input: 'panda.jpg'})\\n  \\n  #t_off_x = tf.get_variable(off_x, name='off_x')\\n  \\n  gradient = tf.gradients(loss, endpoints['layer_1'])\\n  print(tf.get_default_graph())\\n  print(gradient)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 721
        }
      ]
    },
    {
      "metadata": {
        "id": "iPKrDqgnWOZ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b71484b7-6598-4f70-969c-20f37c838f46"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session(graph=tf.get_default_graph())\n",
        "saver.restore(sess,  checkpoint)\n",
        "#init = tf.global_variables_initializer()\n",
        "#sess.run(init)"
      ],
      "execution_count": 722,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from mobilenet_v2_1.0_224.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-YZ8EOdJJmOO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_adversary_noise(image_path, cls_target, noise_limit=3.0,\n",
        "                         required_score=0.99, max_iterations=100):\n",
        "    \"\"\"\n",
        "    Find the noise that must be added to the given image so\n",
        "    that it is classified as the target-class.\n",
        "    \n",
        "    image_path: File-path to the input-image (must be *.jpg).\n",
        "    cls_target: Target class-number (integer between 1-1000).\n",
        "    noise_limit: Limit for pixel-values in the noise.\n",
        "    required_score: Stop when target-class score reaches this.\n",
        "    max_iterations: Max number of optimization iterations to perform.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get predicted output from softmax layer\n",
        "    y_pred = endpoints['Predictions']\n",
        "    \n",
        "    # Create a feed-dict with the image.\n",
        "    #feed_dict = imagenet._create_feed_dict(image_path=image_path)\n",
        "\n",
        "    # Use TensorFlow to calculate the predicted class-scores\n",
        "    # (aka. probabilities) as well as the resized image.\n",
        "    pred, image = sess.run([y_pred, endpoints['layer_1']],\n",
        "                              feed_dict={file_input:image_path})\n",
        "  \n",
        "    #pred = y_pred.eval(feed_dict={file_input: image_path})\n",
        "    #image = Image.open(image_path)\n",
        "    #image = np.asarray(image)\n",
        "  \n",
        "    # Convert to one-dimensional array.\n",
        "    pred = np.squeeze(pred)\n",
        "\n",
        "    # Predicted class-number.\n",
        "    cls_source = np.argmax(pred)\n",
        "\n",
        "    # Score for the predicted class (aka. probability or confidence).\n",
        "    score_source_org = pred.max()\n",
        "\n",
        "    # Names for the source and target classes.\n",
        "    name_source = label_map[cls_source]\n",
        "    name_target = label_map[cls_target]\n",
        "\n",
        "    # Initialize the noise to zero.\n",
        "    noise = 0\n",
        "    \n",
        "    image_dec = np.asarray(PIL.Image.open(image_path))\n",
        "\n",
        "    # Perform a number of optimization iterations to find\n",
        "    # the noise that causes mis-classification of the input image.\n",
        "    for i in range(max_iterations):\n",
        "        print(\"Iteration:\", i)\n",
        "  \n",
        "        # The noisy image is just the sum of the input image and noise.\n",
        "        noisy_image = image + noise\n",
        "\n",
        "        # Ensure the pixel-values of the noisy image are between\n",
        "        # 0 and 255 like a real image. If we allowed pixel-values\n",
        "        # outside this range then maybe the mis-classification would\n",
        "        # be due to this 'illegal' input breaking the Inception model.\n",
        "        noisy_image = np.clip(a=noisy_image, a_min=0.0, a_max=255.0)\n",
        "        \n",
        "        #noisy_image = tf.image.encode_jpeg(noisy_image)\n",
        "        #noisy_image = ','.join(map(str,noisy_image))\n",
        "        \n",
        "        # Create a feed-dict. This feeds the noisy image to the\n",
        "        # tensor in the graph that holds the resized image, because\n",
        "        # this is the final stage for inputting raw image data.\n",
        "        # This also feeds the target class-number that we desire.\n",
        "        feed_dict = {endpoints['layer_1']: noisy_image,\n",
        "                     pl_cls_target: cls_target}\n",
        "\n",
        "        # Calculate the predicted class-scores as well as the gradient.\n",
        "        pred, grad = sess.run([y_pred, gradient],\n",
        "                                 feed_dict=feed_dict)\n",
        "        #pred = y_pred.eval(feed_dict={\n",
        "        #    file_input: noisy_image})\n",
        "        #grad = gradient.eval()\n",
        "\n",
        "        # Convert the predicted class-scores to a one-dim array.\n",
        "        pred = np.squeeze(pred)\n",
        "\n",
        "        # The scores (probabilities) for the source and target classes.\n",
        "        score_source = pred[cls_source]\n",
        "        score_target = pred[cls_target]\n",
        "\n",
        "        # Squeeze the dimensionality for the gradient-array.\n",
        "        grad = np.array(grad).squeeze()\n",
        "\n",
        "        # The gradient now tells us how much we need to change the\n",
        "        # noisy input image in order to move the predicted class\n",
        "        # closer to the desired target-class.\n",
        "\n",
        "        # Calculate the max of the absolute gradient values.\n",
        "        # This is used to calculate the step-size.\n",
        "        grad_absmax = np.abs(grad).max()\n",
        "        \n",
        "        # If the gradient is very small then use a lower limit,\n",
        "        # because we will use it as a divisor.\n",
        "        if grad_absmax < 1e-10:\n",
        "            grad_absmax = 1e-10\n",
        "\n",
        "        # Calculate the step-size for updating the image-noise.\n",
        "        # This ensures that at least one pixel colour is changed by 7.\n",
        "        # Recall that pixel colours can have 255 different values.\n",
        "        # This step-size was found to give fast convergence.\n",
        "        step_size = 7 / grad_absmax\n",
        "\n",
        "        # Print the score etc. for the source-class.\n",
        "        msg = \"Source score: {0:>7.2%}, class-number: {1:>4}, class-name: {2}\"\n",
        "        print(msg.format(score_source, cls_source, name_source))\n",
        "\n",
        "        # Print the score etc. for the target-class.\n",
        "        msg = \"Target score: {0:>7.2%}, class-number: {1:>4}, class-name: {2}\"\n",
        "        print(msg.format(score_target, cls_target, name_target))\n",
        "\n",
        "        # Print statistics for the gradient.\n",
        "        msg = \"Gradient min: {0:>9.6f}, max: {1:>9.6f}, stepsize: {2:>9.2f}\"\n",
        "        print(msg.format(grad.min(), grad.max(), step_size))\n",
        "\n",
        "        # Newline.\n",
        "        print()\n",
        "\n",
        "        # If the score for the target-class is not high enough.\n",
        "        if score_target < required_score:\n",
        "            # Update the image-noise by subtracting the gradient\n",
        "            # scaled by the step-size.\n",
        "            noise -= step_size * grad\n",
        "\n",
        "            # Ensure the noise is within the desired range.\n",
        "            # This avoids distorting the image too much.\n",
        "            noise = np.clip(a=noise,\n",
        "                            a_min=-noise_limit,\n",
        "                            a_max=noise_limit)\n",
        "        else:\n",
        "            # Abort the optimization because the score is high enough.\n",
        "            break\n",
        "\n",
        "    return image.squeeze(), noisy_image.squeeze(), noise, \\\n",
        "           name_source, name_target, \\\n",
        "           score_source, score_source_org, score_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mlCnOdfbS17S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize_image(x):\n",
        "    # Get the min and max values for all pixels in the input.\n",
        "    x_min = x.min()\n",
        "    x_max = x.max()\n",
        "\n",
        "    # Normalize so all values are between 0.0 and 1.0\n",
        "    x_norm = (x - x_min) / (x_max - x_min)\n",
        "\n",
        "    return x_norm\n",
        "  \n",
        "\n",
        "def plot_images(image, noise, noisy_image,\n",
        "                name_source, name_target,\n",
        "                score_source, score_source_org, score_target):\n",
        "    \"\"\"\n",
        "    Plot the image, the noisy image and the noise.\n",
        "    Also shows the class-names and scores.\n",
        "\n",
        "    Note that the noise is amplified to use the full range of\n",
        "    colours, otherwise if the noise is very low it would be\n",
        "    hard to see.\n",
        "\n",
        "    image: Original input image.\n",
        "    noise: Noise that has been added to the image.\n",
        "    noisy_image: Input image + noise.\n",
        "    name_source: Name of the source-class.\n",
        "    name_target: Name of the target-class.\n",
        "    score_source: Score for the source-class.\n",
        "    score_source_org: Original score for the source-class.\n",
        "    score_target: Score for the target-class.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create figure with sub-plots.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(10,10))\n",
        "\n",
        "    # Adjust vertical spacing.\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Use interpolation to smooth pixels?\n",
        "    smooth = True\n",
        "    \n",
        "    # Interpolation type.\n",
        "    if smooth:\n",
        "        interpolation = 'spline16'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    # Plot the original image.\n",
        "    # Note that the pixel-values are normalized to the [0.0, 1.0]\n",
        "    # range by dividing with 255.\n",
        "    ax = axes.flat[0]\n",
        "    ax.imshow(image / 255.0, interpolation=interpolation)\n",
        "    msg = \"Original Image:\\n{0} ({1:.2%})\"\n",
        "    xlabel = msg.format(name_source, score_source_org)\n",
        "    ax.set_xlabel(xlabel)\n",
        "\n",
        "    # Plot the noisy image.\n",
        "    ax = axes.flat[1]\n",
        "    ax.imshow(noisy_image / 255.0, interpolation=interpolation)\n",
        "    msg = \"Image + Noise:\\n{0} ({1:.2%})\\n{2} ({3:.2%})\"\n",
        "    xlabel = msg.format(name_source, score_source, name_target, score_target)\n",
        "    ax.set_xlabel(xlabel)\n",
        "\n",
        "    # Plot the noise.\n",
        "    # The colours are amplified otherwise they would be hard to see.\n",
        "    ax = axes.flat[2]\n",
        "    ax.imshow(normalize_image(noise), interpolation=interpolation)\n",
        "    xlabel = \"Amplified Noise\"\n",
        "    ax.set_xlabel(xlabel)\n",
        "\n",
        "    # Remove ticks from all the plots.\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()\n",
        "    \n",
        "def adversary_example(image_path, cls_target,\n",
        "                      noise_limit, required_score):\n",
        "    \"\"\"\n",
        "    Find and plot adversarial noise for the given image.\n",
        "    \n",
        "    image_path: File-path to the input-image (must be *.jpg).\n",
        "    cls_target: Target class-number (integer between 1-1000).\n",
        "    noise_limit: Limit for pixel-values in the noise.\n",
        "    required_score: Stop when target-class score reaches this.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the adversarial noise.\n",
        "    image, noisy_image, noise, \\\n",
        "    name_source, name_target, \\\n",
        "    score_source, score_source_org, score_target = \\\n",
        "        find_adversary_noise(image_path=image_path,\n",
        "                             cls_target=cls_target,\n",
        "                             noise_limit=noise_limit,\n",
        "                             required_score=required_score)\n",
        "\n",
        "    # Plot the image and the noise.\n",
        "    plot_images(image=image, noise=noise, noisy_image=noisy_image,\n",
        "                name_source=name_source, name_target=name_target,\n",
        "                score_source=score_source,\n",
        "                score_source_org=score_source_org,\n",
        "                score_target=score_target)\n",
        "\n",
        "    # Print some statistics for the noise.\n",
        "    msg = \"Noise min: {0:.3f}, max: {1:.3f}, mean: {2:.3f}, std: {3:.3f}\"\n",
        "    print(msg.format(noise.min(), noise.max(),\n",
        "                     noise.mean(), noise.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZArhqNhsTsuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2625
        },
        "outputId": "c59ae457-b6ff-4740-9cff-677b6065e020"
      },
      "cell_type": "code",
      "source": [
        "image_path = \"panda.jpg\"\n",
        "print(tf.get_default_graph())\n",
        "\n",
        "adversary_example(image_path=image_path,\n",
        "                  cls_target=300,\n",
        "                  noise_limit=3.0,\n",
        "                  required_score=0.99)"
      ],
      "execution_count": 725,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.framework.ops.Graph object at 0x7fef2f17b860>\n",
            "Iteration: 0\n",
            "Source score:  85.00%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:   0.01%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.150164, max:  0.251689, stepsize:     27.81\n",
            "\n",
            "Iteration: 1\n",
            "Source score:   0.02%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:   0.01%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.339750, max:  0.228604, stepsize:     20.60\n",
            "\n",
            "Iteration: 2\n",
            "Source score:   0.18%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:   0.19%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.064060, max:  0.069337, stepsize:    100.96\n",
            "\n",
            "Iteration: 3\n",
            "Source score:   0.32%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:   0.28%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.065595, max:  0.098252, stepsize:     71.25\n",
            "\n",
            "Iteration: 4\n",
            "Source score:   0.14%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:   2.58%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.031026, max:  0.031536, stepsize:    221.97\n",
            "\n",
            "Iteration: 5\n",
            "Source score:   0.09%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:   5.24%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.065189, max:  0.065302, stepsize:    107.19\n",
            "\n",
            "Iteration: 6\n",
            "Source score:   0.05%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  13.05%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.058009, max:  0.056158, stepsize:    120.67\n",
            "\n",
            "Iteration: 7\n",
            "Source score:   0.06%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  56.65%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.030649, max:  0.040715, stepsize:    171.93\n",
            "\n",
            "Iteration: 8\n",
            "Source score:   0.02%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  88.33%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.012015, max:  0.007798, stepsize:    582.62\n",
            "\n",
            "Iteration: 9\n",
            "Source score:   0.02%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  96.23%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.002411, max:  0.001737, stepsize:   2903.10\n",
            "\n",
            "Iteration: 10\n",
            "Source score:   0.01%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  94.53%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.002281, max:  0.002416, stepsize:   2896.90\n",
            "\n",
            "Iteration: 11\n",
            "Source score:   0.01%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  94.31%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.001657, max:  0.001610, stepsize:   4225.14\n",
            "\n",
            "Iteration: 12\n",
            "Source score:   0.00%, class-number:  389, class-name: giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\n",
            "Target score:  99.26%, class-number:  300, class-name: meerkat, mierkat\n",
            "Gradient min: -0.000465, max:  0.000504, stepsize:  13875.57\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-725-2bb4da883d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mcls_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mnoise_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                   required_score=0.99)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-724-961e8b7880a2>\u001b[0m in \u001b[0;36madversary_example\u001b[0;34m(image_path, cls_target, noise_limit, required_score)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mscore_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mscore_source_org\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_source_org\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 score_target=score_target)\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Print some statistics for the noise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-724-961e8b7880a2>\u001b[0m in \u001b[0;36mplot_images\u001b[0;34m(image, noise, noisy_image, name_source, name_target, score_source, score_source_org, score_target)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# range by dividing with 255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Original Image:\\n{0} ({1:.2%})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mxlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_source_org\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5129\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5131\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5132\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    620\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    621\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAJDCAYAAADXd2qEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb6/pxojZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fef41ba2f60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "H5GgLi7tv-Qx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "06f24f45-28f5-4a0a-8d56-dbb28d8354a4"
      },
      "cell_type": "code",
      "source": [
        "print(noisy_image)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d855e1fefc5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'noisy_image' is not defined"
          ]
        }
      ]
    }
  ]
}